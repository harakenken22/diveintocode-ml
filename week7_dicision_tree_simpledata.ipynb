{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "class ScratchDecesionTreeClassifierDepth1():\n",
    "    \"\"\"\n",
    "    深さ1の決定木分類器のスクラッチ実装\n",
    "    Parameters\n",
    "    ----------\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.gini = None\n",
    "        self.verbose = verbose\n",
    "        self.best_ig = None\n",
    "        self.best_feature = None\n",
    "        self.best_threshold = None\n",
    "        self.label_left = None\n",
    "        self.label_right = None\n",
    "        \n",
    "    def gini_score(self, target):\n",
    "        classes = np.unique(target)\n",
    "        total_num = target.shape[0]\n",
    "    \n",
    "        gini = 1.0\n",
    "        for c in classes:\n",
    "#             print(c)\n",
    "            gini -= (len(target[target==c])/total_num) ** 2.0 #ジニ不純度の計算\n",
    "#             print(gini)\n",
    "        \n",
    "        return gini\n",
    "    \n",
    "    \n",
    "#     def build(data, target):\n",
    "#         #対象とするサンプルの個数\n",
    "#         total_num = data.shape[0] #シェイプの行の個数を取得\n",
    "#         feature_num = data.shape[1] #シェイプで特徴量の個数を取得\n",
    "    \n",
    "#         #ベストな分割を記憶しておくための変数の用意(これを更新していく)\n",
    "#         best_ig = 0 #情報利用初期値\n",
    "#         best_feature = None #最高の特徴量の初期値\n",
    "#         best_threshold = None #最高の特徴量の初期値\n",
    "    \n",
    "# #         self.gini = gini_score(target) #ジニ不純を求める\n",
    "    \n",
    "#         for f in range(feature_num): #特徴量の数だけfor文を回す\n",
    "        \n",
    "#             #分割候補の計算\n",
    "#             data_f = np.unique(data[:, f]) #f番目の特徴量(重複削除)\n",
    "        \n",
    "#             for threshold in data_f:\n",
    "            \n",
    "#                 #閾値で２グループに分割\n",
    "#                 target_left = target[data[:,f] < threshold] #dataのインデックスからターゲットのインデックスを参照して取得する。\n",
    "#                 target_right = target[data[:,f] >= threshold] #dataのインデックスからターゲットのインデックスを参照して取得する。\n",
    "            \n",
    "#                 #分割後の不純度から情報利得(information gain)を計算\n",
    "#                 gini_left = gini_score(target_left) #左側のGINIスコアを計算\n",
    "#                 gini_right = gini_score(target_right) #右側のGINIスコアを計算\n",
    "#                 p_left = float(target_left.shape[0]) / total_num #左側に分類されたサンプル数と全体のサンプル数\n",
    "#                 p_right = float(target_right.shape[0]) / total_num #右側に分類されたサンプル数と全体のサンプル数\n",
    "#                 ig = gini - (p_left * gini_left + p_right * gini_right) #情報利得IGを求める(両側のサンプル数とジニ不純度の掛け合わせ)\n",
    "            \n",
    "#                 #最も良い分割であれば更新して保持\n",
    "#                 if ig > best_ig: #出てきたGINI不純度を更新\n",
    "#                     self.best_ig = ig\n",
    "#                     self.best_feature = f #一番よかったときの特徴量を記録 \n",
    "#                     self.best_threshold = threshold #一番よかったときの閾値を保存\n",
    "                \n",
    "#             #情報利得が増えなければ終了(現在のノードを葉ノードにする)\n",
    "#             if best_ig == 0:\n",
    "#                 return\n",
    "        \n",
    "        \n",
    "    def fit(self, data, target):\n",
    "        #対象とするサンプルの個数\n",
    "        total_num = data.shape[0] #シェイプの行の個数を取得\n",
    "        feature_num = data.shape[1] #シェイプで特徴量の個数を取得\n",
    "    \n",
    "        #ベストな分割を記憶しておくための変数の用意(これを更新していく)\n",
    "        self.best_ig = 0 #情報利用初期値\n",
    "        self.best_feature = None #最高の特徴量の初期値\n",
    "        self.best_threshold = None #最高の特徴量の初期値\n",
    "    \n",
    "        self.gini = self.gini_score(target) #ジニ不純を求める\n",
    "    \n",
    "        for f in range(feature_num): #特徴量の数だけfor文を回す\n",
    "        \n",
    "            #分割候補の計算\n",
    "            data_f = np.unique(data[:, f]) #f番目の特徴量(重複削除)\n",
    "        \n",
    "            for threshold in data_f:\n",
    "            \n",
    "                #閾値で２グループに分割\n",
    "                target_left = target[data[:,f] < threshold] #dataのインデックスからターゲットのインデックスを参照して取得する。\n",
    "                target_right = target[data[:,f] >= threshold] #dataのインデックスからターゲットのインデックスを参照して取得する。\n",
    "            \n",
    "                #分割後の不純度から情報利得(information gain)を計算\n",
    "                gini_left = self.gini_score(target_left) #左側のGINIスコアを計算\n",
    "                gini_right = self.gini_score(target_right) #右側のGINIスコアを計算\n",
    "                p_left = float(target_left.shape[0]) / total_num #左側に分類されたサンプル数と全体のサンプル数\n",
    "                p_right = float(target_right.shape[0]) / total_num #右側に分類されたサンプル数と全体のサンプル数\n",
    "                ig = self.gini - (p_left * gini_left + p_right * gini_right) #情報利得IGを求める(両側のサンプル数とジニ不純度の掛け合わせ)\n",
    "            \n",
    "                #最も良い分割であれば更新して保持\n",
    "                if ig > self.best_ig: #出てきたGINI不純度を更新\n",
    "                    self.best_ig = ig\n",
    "                    self.best_feature = f #一番よかったときの特徴量を記録 \n",
    "                    self.best_threshold = threshold #一番よかったときの閾値を保存\n",
    "                \n",
    "            #情報利得が増えなければ終了(現在のノードを葉ノードにする)\n",
    "#             if best_ig == 0:\n",
    "#                 return\n",
    "        print('Best_threshold:{}'.format(self.best_threshold))\n",
    "        print('Best_feature:{}'.format(self.best_feature))\n",
    "        \n",
    "        terget_left = target[data[:, self.best_feature] < self.best_threshold]\n",
    "        print ('terget_left{}'.format(terget_left))\n",
    "        \n",
    "        terget_right = target[data[:, self.best_feature] >= self.best_threshold]\n",
    "        print ('terget_right{}'.format(terget_right))\n",
    "        print('-----------------------------------')\n",
    "        \n",
    "        self.label_left = stats.mode(terget_left)[0][0]\n",
    "        self.label_right = stats.mode(terget_right)[0][0]\n",
    "        \"\"\"\n",
    "        決定木分類器を学習する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        \"\"\"\n",
    "#         if self.verbose:\n",
    "#             #verboseをTrueにした際は学習過程を出力\n",
    "# #             print()\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "    def predict_tree(self, d):\n",
    "        if d[self.best_feature] < self.best_threshold:\n",
    "            return self.label_left\n",
    "        else:\n",
    "            return self.label_right\n",
    "    \n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        決定木分類器を使いラベルを推定する\n",
    "        \"\"\"\n",
    "        \n",
    "        ans = []\n",
    "        \n",
    "        for d in data:\n",
    "            label = self.predict_tree(d)# ルートノードでクラス予測したラベルが返ってくる\n",
    "#             print(label)\n",
    "            ans.append(label)\n",
    "        return np.array(ans)\n",
    "    \n",
    "    def print_tree(self, depth, TF):\n",
    "        \"\"\"分類条件を出力する\"\"\"\n",
    "\n",
    "        head = \"    \" * depth + TF + \" -> \"\n",
    "\n",
    "        # 節の場合\n",
    "        if self.feature != None:\n",
    "            print(head + str(self.best_feature) + \" < \" + str(self.best_threshold) + \"?\")\n",
    "            self.left.print_tree(depth + 1, \"T\")\n",
    "            self.right.print_tree(depth + 1, \"F\")\n",
    "\n",
    "        # 葉の場合\n",
    "        else:\n",
    "            print(head + \"{\" + str(self.label) + \": \" + str(self.total_num) + \"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple = np.array([[-0.44699 , -2.8073  ],[-1.4621  , -2.4586  ],\n",
    "       [ 0.10645 ,  1.9242  ],[-3.5944  , -4.0112  ],\n",
    "       [-0.9888  ,  4.5718  ],[-3.1625  , -3.9606  ],\n",
    "       [ 0.56421 ,  0.72888 ],[-0.60216 ,  8.4636  ],\n",
    "       [-0.61251 , -0.75345 ],[-0.73535 , -2.2718  ],\n",
    "       [-0.80647 , -2.2135  ],[ 0.86291 ,  2.3946  ],\n",
    "       [-3.1108  ,  0.15394 ],[-2.9362  ,  2.5462  ],\n",
    "       [-0.57242 , -2.9915  ],[ 1.4771  ,  3.4896  ],\n",
    "       [ 0.58619 ,  0.37158 ],[ 0.6017  ,  4.3439  ],\n",
    "       [-2.1086  ,  8.3428  ],[-4.1013  , -4.353   ],\n",
    "       [-1.9948  , -1.3927  ],[ 0.35084 , -0.031994],\n",
    "       [ 0.96765 ,  7.8929  ],[-1.281   , 15.6824  ],\n",
    "       [ 0.96765 , 10.083   ],[ 1.3763  ,  1.3347  ],\n",
    "       [-2.234   , -2.5323  ],[-2.9452  , -1.8219  ],\n",
    "       [ 0.14654 , -0.28733 ],[ 0.5461  ,  5.8245  ],\n",
    "       [-0.65259 ,  9.3444  ],[ 0.59912 ,  5.3524  ],\n",
    "       [ 0.50214 , -0.31818 ],[-3.0603  , -3.6461  ],\n",
    "       [-6.6797  ,  0.67661 ],[-2.353   , -0.72261 ],\n",
    "       [ 1.1319  ,  2.4023  ],[-0.12243 ,  9.0162  ],\n",
    "       [-2.5677  , 13.1779  ],[ 0.057313,  5.4681  ]])\n",
    "y_simple = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_threshold:5.3524\n",
      "Best_feature:1\n",
      "terget_left[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "terget_right[0 0 1 1 1 1 1 1 1 1 1]\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "tree_scrach_simple = ScratchDecesionTreeClassifierDepth1()\n",
    "tree_scrach_simple.fit(X_simple, y_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_simple_predict = tree_scrach_simple.predict(X_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[18  2]\n",
      " [11  9]]\n",
      "accuracy =  0.675\n",
      "precision =  0.8181818181818182\n",
      "recall =  0.45\n",
      "f1 score =  0.5806451612903226\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_simple, y_pred=y_simple_predict))\n",
    "print('accuracy = ', accuracy_score(y_true=y_simple, y_pred=y_simple_predict))\n",
    "print('precision = ', precision_score(y_true=y_simple, y_pred=y_simple_predict))\n",
    "print('recall = ', recall_score(y_true=y_simple, y_pred=y_simple_predict))\n",
    "print('f1 score = ', f1_score(y_true=y_simple, y_pred=y_simple_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
